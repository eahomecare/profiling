import {
  Injectable,
  Logger,
} from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { ChatOpenAI } from 'langchain/chat_models/openai';
import {
  HumanMessage,
  SystemMessage,
} from 'langchain/schema';

@Injectable()
export class AiEngineService {
  private chat: ChatOpenAI;
  private readonly logger = new Logger(
    AiEngineService.name,
  );

  constructor(
    private configService: ConfigService,
  ) {
    const openAIKey = this.configService.get(
      'OPEN_AI_KEY',
    );
    if (!openAIKey) {
      this.logger.error(
        'OpenAI key not found in the configuration.',
      );
      throw new Error('OpenAI key not found.');
    }
    this.chat = new ChatOpenAI({
      openAIApiKey: openAIKey,
      modelName:
        'ft:gpt-3.5-turbo-0613:personal::8HX9SF8g',
      temperature: 0.8,
    });
  }

  async processServiceInformation(
    levelRequired: number,
    category: string,
    serviceTitle: string,
    serviceDescription: string,
    pastKeywords: {
      key: string;
      level: string;
    }[],
    pastQuestionsAnswers: {
      level: number;
      category: string;
      question: string;
      possibleOptions: string[];
      answersSelected: string[];
    }[],
  ) {
    try {
      this.logger.log(
        'Starting AI processing of service information.',
      );

      const userMessage = JSON.stringify({
        'level required': levelRequired,
        'category required': category,
        'service title': serviceTitle,
        'service description': serviceDescription,
        'past keywords': pastKeywords,
        'past questions and answers selected':
          pastQuestionsAnswers,
      });

      const systemMessage = `
        assistant generates questions used to profile customers based on their past keywords, 
        past questions answered and their service booking type. If service details are available, 
        and levelRequired is 4 or 5, then the question assistant produces should tend to enquire 
        about interests towards the available services considering the past data tendencies of the 
        categoryRequired context. The questions asked will have levels from 1 to 5 and category 
        based on the levelRequired and categoryRequired fields respectively. The assistant's 
        question's context will consider all data so as not to be repetitive. The number of possible 
        answers can vary to cover a broad scope but the last answer should express disinterest, 
        e.g.: 'No' or 'None'. All the possible answers generated by assistant should be keywords 
        and not sentences.
      `.trim();

      this.logger.log(
        'Generated prompt strings for AI model.',
      );

      function isValidResponse(content: any) {
        const hasValidLevel =
          typeof content.level === 'number';
        const hasValidCategory =
          typeof content.category === 'string' &&
          content.category.trim() !== '';
        const hasValidText =
          typeof content.text === 'string' &&
          content.text.trim() !== '';
        const hasValidAnswers =
          content.hasOwnProperty('Answers') &&
          Array.isArray(content.Answers) &&
          content.Answers.length > 0 &&
          content.Answers.every(
            (answer: string) =>
              typeof answer === 'string' &&
              answer.trim() !== '',
          );

        return (
          hasValidLevel &&
          hasValidCategory &&
          hasValidText &&
          hasValidAnswers
        );
      }

      function processResponse(response: any) {
        const processedResponse = { ...response };

        for (const key in processedResponse) {
          if (
            processedResponse.hasOwnProperty(key)
          ) {
            if (
              Array.isArray(
                processedResponse[key],
              )
            ) {
              processedResponse.Answers =
                processedResponse[key];
              delete processedResponse[key];
            } else if (
              typeof processedResponse[key] ===
                'string' &&
              key
                .replace(/\s+/g, '')
                .toLowerCase() === 'question'
            ) {
              processedResponse.text =
                processedResponse[key];
              delete processedResponse[key];
            }
          }
        }

        return processedResponse;
      }

      const getValidResponse = async () => {
        let validResponse: boolean;
        let attempts = 0;
        const maxAttempts = 3;

        do {
          const response = await this.chat.call([
            new SystemMessage(systemMessage),
            new HumanMessage(userMessage, {
              role: 'user',
            }),
          ]);

          this.logger.log(
            'Received response from AI model.',
          );

          if (!('lc_kwargs' in response)) {
            throw new Error(
              'Response does not contain kwargs',
            );
          }

          const aiResponse = response as any;

          let content: any;
          if (
            typeof aiResponse.lc_kwargs
              .content === 'string'
          ) {
            content = JSON.parse(
              aiResponse.lc_kwargs.content,
            );
          } else {
            throw new Error(
              'Content is not a string',
            );
          }

          const processedResponse =
            processResponse(content);

          if (
            isValidResponse(processedResponse)
          ) {
            validResponse = processedResponse;
            break;
          }

          attempts++;
        } while (
          attempts < maxAttempts &&
          !validResponse
        );

        if (!validResponse) {
          throw new Error(
            'Unable to generate a valid response after multiple attempts.',
          );
        }

        return validResponse;
      };

      const finalResponse =
        await getValidResponse();
      return finalResponse;
    } catch (error) {
      this.logger.error(
        'An error occurred during AI processing.',
        error.stack,
      );
      throw error;
    }
  }
}
